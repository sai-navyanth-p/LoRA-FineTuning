{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jrYMGAvqWgDw"
      },
      "outputs": [],
      "source": [
        "os.environ[\"TRANSFORMERS_OFFLINE\"] = \"1\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "c620d4839a024841b2b5de0f5fa6e2ad"
          ]
        },
        "id": "-Wdi1xxIWgDx",
        "outputId": "dcaa1c53-69ed-4588-b124-f3883a7e2603"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-04-21 17:46:41.953406: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-04-21 17:46:43.490711: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1745272004.015069    5483 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1745272004.170681    5483 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1745272005.452690    5483 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1745272005.452733    5483 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1745272005.452735    5483 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1745272005.452737    5483 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-04-21 17:46:45.580623: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "INFO:__main__:Freezing base layers\n",
            "INFO:__main__:Trainable parameters: 667396\n",
            "/home/vn2263/.local/lib/python3.9/site-packages/transformers/training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "/tmp/ipykernel_5483/389988502.py:232: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n",
            "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Failed to detect the name of this notebook. You can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msp8138\u001b[0m (\u001b[33msp8138-new-york-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.9"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/scratch/vn2263/dl-project2/wandb/run-20250421_174745-xjrlob5d</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/sp8138-new-york-university/huggingface/runs/xjrlob5d' target=\"_blank\">./trained_models/my_best_model</a></strong> to <a href='https://wandb.ai/sp8138-new-york-university/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/sp8138-new-york-university/huggingface' target=\"_blank\">https://wandb.ai/sp8138-new-york-university/huggingface</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/sp8138-new-york-university/huggingface/runs/xjrlob5d' target=\"_blank\">https://wandb.ai/sp8138-new-york-university/huggingface/runs/xjrlob5d</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='3750' max='3750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [3750/3750 04:53, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>1.376400</td>\n",
              "      <td>1.371926</td>\n",
              "      <td>0.356184</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>1.333100</td>\n",
              "      <td>1.327090</td>\n",
              "      <td>0.811579</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>1.213800</td>\n",
              "      <td>1.193648</td>\n",
              "      <td>0.864868</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>0.903400</td>\n",
              "      <td>0.869584</td>\n",
              "      <td>0.875526</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2500</td>\n",
              "      <td>0.567500</td>\n",
              "      <td>0.523323</td>\n",
              "      <td>0.877895</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3000</td>\n",
              "      <td>0.452800</td>\n",
              "      <td>0.423279</td>\n",
              "      <td>0.881447</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3500</td>\n",
              "      <td>0.433500</td>\n",
              "      <td>0.398339</td>\n",
              "      <td>0.881053</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:__main__:Starting inference loop\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c620d4839a024841b2b5de0f5fa6e2ad",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Evaluating:   0%|          | 0/119 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:__main__:Writing results to disk\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import pickle\n",
        "import logging\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import evaluate\n",
        "import nlpaug.augmenter.word as naw\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm.auto import tqdm\n",
        "from torch import nn\n",
        "\n",
        "from datasets import load_dataset, Dataset, ClassLabel\n",
        "from transformers import (\n",
        "    RobertaTokenizer, RobertaModel, RobertaForSequenceClassification,\n",
        "    Trainer, TrainingArguments, EarlyStoppingCallback, DataCollatorWithPadding,\n",
        "    RobertaPreTrainedModel, AutoConfig\n",
        ")\n",
        "from peft import LoraConfig, get_peft_model\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "log = logging.getLogger(__name__)\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Global hyperparameters and switches\n",
        "class Settings:\n",
        "    MODEL_NAME = \"roberta-base\"\n",
        "    SAVE_DIR = \"my_best_model\"\n",
        "    AUGMENT = False\n",
        "    EARLY_STOP = True\n",
        "    FNN_ENABLED = False\n",
        "    FREEZE_MODEL = True\n",
        "    WEIGHT_DECAY_ON = False\n",
        "    MC_DROPOUT = False\n",
        "    MAX_LENGTH = 64\n",
        "    TRAIN_BATCH = 32\n",
        "    EVAL_BATCH = 64\n",
        "    EPOCHS = 1\n",
        "    LEARNING_RATE = 5e-6\n",
        "    EARLY_STOP_PATIENCE = 5\n",
        "    DROPOUT_ITER = 10\n",
        "    TRAIN_LAST_LAYERS = 2\n",
        "\n",
        "    LORA_CFG = dict(\n",
        "        r=3,\n",
        "        lora_alpha=6,\n",
        "        lora_dropout=0.05,\n",
        "        bias=\"none\",\n",
        "        target_modules=[\"query\", \"value\"],\n",
        "        task_type=\"SEQ_CLS\"\n",
        "    )\n",
        "\n",
        "# Custom Roberta with additional feed-forward layers\n",
        "class ExtendedRoberta(RobertaPreTrainedModel):\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "        self.roberta = RobertaModel(config)\n",
        "        self.fnn = nn.Sequential(\n",
        "            nn.Linear(config.hidden_size, 512), nn.GELU(), nn.Dropout(0.2),\n",
        "            nn.Linear(512, 512), nn.GELU(), nn.Dropout(0.2),\n",
        "            nn.Linear(512, 256), nn.GELU(), nn.Dropout(0.2),\n",
        "            nn.Linear(256, config.num_labels)\n",
        "        )\n",
        "        self.num_labels = config.num_labels\n",
        "        self.init_weights()\n",
        "\n",
        "    def forward(self, input_ids=None, attention_mask=None, labels=None, **kwargs):\n",
        "        pooled = self.roberta(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state[:, 0]\n",
        "        logits = self.fnn(pooled)\n",
        "        if labels is not None:\n",
        "            loss_fn = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
        "            return {\"loss\": loss_fn(logits, labels), \"logits\": logits}\n",
        "        return {\"logits\": logits}\n",
        "\n",
        "# Preprocessing helpers\n",
        "def tokenize_set(tokenizer, data, length):\n",
        "    def map_fn(batch):\n",
        "        return tokenizer(batch[\"text\"], truncation=True, padding=\"max_length\", max_length=length)\n",
        "    return data.map(map_fn, batched=True, remove_columns=[\"text\"])\n",
        "\n",
        "def augment_train(data):\n",
        "    augmenter = naw.SynonymAug(aug_src='wordnet')\n",
        "    def augment_fn(example):\n",
        "        try:\n",
        "            aug = augmenter.augment(example[\"text\"])\n",
        "            return {\"text\": aug[0] if isinstance(aug, list) else aug}\n",
        "        except:\n",
        "            return example\n",
        "    aug_data = data[\"train\"].map(augment_fn)\n",
        "    combined = Dataset.from_list(data[\"train\"].to_list() + aug_data.to_list())\n",
        "    return combined, data[\"test\"]\n",
        "\n",
        "# Model evaluation metric\n",
        "def compute_accuracy(pred):\n",
        "    predictions, labels = pred\n",
        "    predictions = np.argmax(predictions, axis=1)\n",
        "    return evaluate.load(\"accuracy\").compute(predictions=predictions, references=labels)\n",
        "\n",
        "# Monte Carlo Dropout prediction function\n",
        "def mc_dropout(model, dataset, collator, device, repeat):\n",
        "    model.train()\n",
        "    loader = DataLoader(dataset, batch_size=64, collate_fn=collator)\n",
        "    predictions = []\n",
        "    for _ in range(repeat):\n",
        "        iter_preds = []\n",
        "        for batch in loader:\n",
        "            inputs = {k: v.to(device) for k, v in batch.items()}\n",
        "            with torch.no_grad():\n",
        "                out = model(**inputs)\n",
        "            iter_preds.append(out.logits.cpu().numpy())\n",
        "        predictions.append(np.concatenate(iter_preds, axis=0))\n",
        "    return np.argmax(np.mean(predictions, axis=0), axis=1)\n",
        "\n",
        "# Freeze all except LoRA and classifier\n",
        "def lock_parameters(model):\n",
        "    log.info(\"Freezing base layers\")\n",
        "    for name, param in model.named_parameters():\n",
        "        if \"lora\" not in name and \"classifier\" not in name:\n",
        "            param.requires_grad = False\n",
        "\n",
        "# Model loading and prediction helpers\n",
        "def fetch_model_and_tokenizer(path):\n",
        "    log.info(\"Fetching model and tokenizer\")\n",
        "    tokenizer = RobertaTokenizer.from_pretrained(f\"./trained_models/{path}/final_model\")\n",
        "    model = RobertaForSequenceClassification.from_pretrained(\n",
        "        f\"./trained_models/{path}/final_model\", num_labels=4\n",
        "    ).to(DEVICE)\n",
        "    model.eval()\n",
        "    return tokenizer, model\n",
        "\n",
        "\n",
        "def retrieve_test_data(path):\n",
        "    with open(path, \"rb\") as file:\n",
        "        return pickle.load(file)\n",
        "\n",
        "def encode_custom_data(data, tokenizer):\n",
        "    def enc_fn(batch):\n",
        "        return tokenizer(batch[\"text\"], truncation=True, max_length=128, padding=\"max_length\")\n",
        "    return data.map(enc_fn, batched=True, remove_columns=[\"text\"])\n",
        "\n",
        "def infer(model, data_loader):\n",
        "    log.info(\"Starting inference loop\")\n",
        "    preds = []\n",
        "    for batch in tqdm(data_loader, desc=\"Evaluating\"):\n",
        "        batch = {k: v.to(DEVICE) for k, v in batch.items()}\n",
        "        with torch.no_grad():\n",
        "            logits = model(**batch).logits\n",
        "        preds.append(logits.argmax(dim=1).cpu())\n",
        "    return torch.cat(preds, dim=0)\n",
        "\n",
        "def export_to_csv(predictions, path):\n",
        "    log.info(\"Writing results to disk\")\n",
        "    pd.DataFrame({\"ID\": list(range(len(predictions))), \"Label\": predictions.numpy()}).to_csv(path, index=False)\n",
        "\n",
        "# Main pipeline\n",
        "def run_pipeline():\n",
        "    cfg = Settings()\n",
        "    tokenizer = RobertaTokenizer.from_pretrained(cfg.MODEL_NAME)\n",
        "    tokenizer.model_max_length = cfg.MAX_LENGTH\n",
        "\n",
        "    if cfg.AUGMENT:\n",
        "        train_data, test_data = augment_train(load_dataset(\"ag_news\"))\n",
        "    else:\n",
        "        raw = load_dataset(\"ag_news\")\n",
        "        train_data, test_data = raw[\"train\"], raw[\"test\"]\n",
        "\n",
        "    train_encoded = tokenize_set(tokenizer, train_data, cfg.MAX_LENGTH).rename_column(\"label\", \"labels\")\n",
        "    test_encoded = tokenize_set(tokenizer, test_data, cfg.MAX_LENGTH).rename_column(\"label\", \"labels\")\n",
        "\n",
        "    if isinstance(train_encoded.features[\"labels\"], ClassLabel):\n",
        "        labels = train_encoded.features[\"labels\"].names\n",
        "    else:\n",
        "        labels = [\"World\", \"Sports\", \"Business\", \"Sci/Tech\"]\n",
        "\n",
        "    num_labels = len(set(train_encoded[\"labels\"]))\n",
        "    label_map = {i: name for i, name in enumerate(labels)}\n",
        "    reverse_map = {v: k for k, v in label_map.items()}\n",
        "\n",
        "    if cfg.FNN_ENABLED:\n",
        "        conf = AutoConfig.from_pretrained(cfg.MODEL_NAME, num_labels=num_labels)\n",
        "        model = ExtendedRoberta.from_pretrained(cfg.MODEL_NAME, config=conf)\n",
        "    else:\n",
        "        model = RobertaForSequenceClassification.from_pretrained(\n",
        "            cfg.MODEL_NAME, num_labels=num_labels, id2label=label_map, label2id=reverse_map\n",
        "        )\n",
        "\n",
        "    model = get_peft_model(model, LoraConfig(**cfg.LORA_CFG))\n",
        "    if cfg.FREEZE_MODEL:\n",
        "        lock_parameters(model)\n",
        "\n",
        "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    assert trainable < 1_000_000, f\"Too many trainable parameters: {trainable}\"\n",
        "    log.info(f\"Trainable parameters: {trainable}\")\n",
        "\n",
        "    args = TrainingArguments(\n",
        "        output_dir=f\"./trained_models/{cfg.SAVE_DIR}\",\n",
        "        evaluation_strategy=\"steps\",\n",
        "        save_strategy=\"steps\",\n",
        "        eval_steps=500,\n",
        "        save_steps=4000,\n",
        "        learning_rate=cfg.LEARNING_RATE,\n",
        "        per_device_train_batch_size=cfg.TRAIN_BATCH,\n",
        "        per_device_eval_batch_size=cfg.EVAL_BATCH,\n",
        "        num_train_epochs=cfg.EPOCHS,\n",
        "        weight_decay=cfg.WEIGHT_DECAY_ON * 0.01,\n",
        "        logging_dir=\"./logs\",\n",
        "        logging_steps=100,\n",
        "        save_total_limit=3,\n",
        "        load_best_model_at_end=True,\n",
        "        metric_for_best_model=\"accuracy\",\n",
        "        greater_is_better=True,\n",
        "        lr_scheduler_type=\"linear\",\n",
        "        warmup_ratio=0.1,\n",
        "        report_to=\"wandb\",\n",
        "    )\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=args,\n",
        "        train_dataset=train_encoded,\n",
        "        eval_dataset=test_encoded,\n",
        "        tokenizer=tokenizer,\n",
        "        data_collator=DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=\"pt\"),\n",
        "        compute_metrics=compute_accuracy,\n",
        "        callbacks=[EarlyStoppingCallback(early_stopping_patience=cfg.EARLY_STOP_PATIENCE)] if cfg.EARLY_STOP else []\n",
        "    )\n",
        "\n",
        "    trainer.train()\n",
        "\n",
        "    # === Inference and Save Predictions ===\n",
        "    test_loader = DataLoader(\n",
        "        test_encoded.remove_columns(\"labels\"),\n",
        "        batch_size=cfg.EVAL_BATCH,\n",
        "        collate_fn=DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=\"pt\")\n",
        "    )\n",
        "    model.eval()\n",
        "    predictions = infer(model, test_loader)\n",
        "    export_to_csv(predictions, \"predictions.csv\")\n",
        "\n",
        "\n",
        "\n",
        "# Run the main pipeline when this script is executed\n",
        "def main():\n",
        "    run_pipeline()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ukOmCgZWgDy"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}